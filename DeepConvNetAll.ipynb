{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Permute\n",
    "from keras.layers import ConvLSTM2D, Conv2D, MaxPooling2D\n",
    "from keras.layers import MaxPooling1D, Conv1D\n",
    "from keras.layers import GRU, LSTM, BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Flatten, Reshape, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Activation\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from UtilNNDL import create_window_data\n",
    "from UtilNNDL import plot_hist\n",
    "from UtilNNDL import prepare_data\n",
    "from UtilNNDL import plot_confusion_matrix\n",
    "from UtilNNDL import bandpass_cnt\n",
    "from UtilNNDL import exponential_running_standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the data by taking out nans and dividing into test and train\n",
    "file_path = 'datasets/'\n",
    "file_path = '/home/carla/Downloads/project_datasets/project_datasets/'\n",
    "train_data, test_data, train_labels, test_labels = prepare_data(file_path, \n",
    "                                                                num_test_samples = 50, \n",
    "                                                                verbose= False, \n",
    "                                                                return_all=True,\n",
    "                                                                num_files =9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2108, 1000, 22)\n",
      "(2108, 4)\n",
      "(50, 1000, 22)\n",
      "(50, 4)\n"
     ]
    }
   ],
   "source": [
    "print train_data.shape\n",
    "print train_labels.shape\n",
    "\n",
    "# Test on the first 50 samples of subject 1\n",
    "test_data = test_data[0:50]\n",
    "test_labels = test_labels[0:50]\n",
    "\n",
    "print test_data.shape\n",
    "print test_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2108, 22, 1000)\n",
      "(50, 22, 1000)\n",
      "(2108, 22, 1000)\n",
      "(50, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "#assist numerical stability\n",
    "train_data = train_data*(1e6)\n",
    "test_data = test_data*(1e6)\n",
    "train_data = train_data.swapaxes(1,2)\n",
    "test_data = test_data.swapaxes(1,2)\n",
    "print train_data.shape\n",
    "print test_data.shape\n",
    "for i,a in enumerate(train_data):\n",
    "    train_data[i] = bandpass_cnt(a, 4, 38, 250, filt_order=3)\n",
    "for i,a in enumerate(test_data):\n",
    "    test_data[i] = bandpass_cnt(a, 4, 38, 250, filt_order=3)    \n",
    "print train_data.shape\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2108, 22, 1000)\n",
      "(50, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Standardize the data\n",
    "for i,a in enumerate(train_data):\n",
    "    train_data[i] = exponential_running_standardize(a, factor_new=0.001, init_block_size=1000, eps=1e-4)\n",
    "for i,a in enumerate(test_data):\n",
    "    test_data[i] = exponential_running_standardize(a, factor_new=0.001, init_block_size=1000, eps=1e-4)\n",
    "train_data = train_data.swapaxes(1,2)\n",
    "test_data = test_data.swapaxes(1,2) \n",
    "print train_data.shape\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21080, 22, 512)\n",
      "(21080, 4)\n",
      "(500, 22, 512)\n",
      "(500, 4)\n"
     ]
    }
   ],
   "source": [
    "#Augment the data into a bigger set by windowing\n",
    "train_data_sliced, train_labels_sliced = create_window_data(train_data, train_labels, windows=10)\n",
    "test_data_sliced, test_labels_sliced = create_window_data(test_data, test_labels, windows=10)\n",
    "print train_data_sliced.shape\n",
    "print train_labels_sliced.shape\n",
    "print test_data_sliced.shape\n",
    "print test_labels_sliced.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 22, 512)\n",
      "(500, 4)\n"
     ]
    }
   ],
   "source": [
    "test_data_sliced = test_data_sliced[:500,:]\n",
    "test_labels_sliced = test_labels_sliced[:500,:]\n",
    "print test_data_sliced.shape\n",
    "print test_labels_sliced.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21080, 512, 22, 1)\n",
      "(500, 512, 22, 1)\n"
     ]
    }
   ],
   "source": [
    "traindata2d = train_data_sliced.reshape(train_data_sliced.shape[0], train_data_sliced.shape[2], train_data_sliced.shape[1], 1)\n",
    "testdata2d = test_data_sliced.reshape(test_data_sliced.shape[0], test_data_sliced.shape[2], test_data_sliced.shape[1], 1)\n",
    "print traindata2d.shape\n",
    "print testdata2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out:\n",
    "    * X batchnorm before nonlinearity\n",
    "    * X regularization\n",
    "    * dropout 0.5 applied to inputs to conv layers after the first\n",
    "    * “new tied loss function” for further regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 493, 22, 20)       420       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 479, 3, 20)        120020    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 479, 3, 20)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 159, 3, 20)        0         \n",
      "_________________________________________________________________\n",
      "permute_10 (Permute)         (None, 159, 20, 3)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 159, 20, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 150, 1, 40)        24040     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 150, 1, 40)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 50, 1, 40)         0         \n",
      "_________________________________________________________________\n",
      "permute_11 (Permute)         (None, 50, 40, 1)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 50, 40, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 41, 1, 80)         32080     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 41, 1, 80)         320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 13, 1, 80)         0         \n",
      "_________________________________________________________________\n",
      "permute_12 (Permute)         (None, 13, 80, 1)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 13, 80, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 4, 1, 160)         128160    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4, 1, 160)         640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 1, 1, 160)         0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 160, 1)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 644       \n",
      "=================================================================\n",
      "Total params: 306,564\n",
      "Trainable params: 305,964\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "Train on 15810 samples, validate on 5270 samples\n",
      "Epoch 1/30\n",
      "15810/15810 [==============================] - 26s 2ms/step - loss: 1.4693 - acc: 0.2952 - val_loss: 1.5169 - val_acc: 0.2844\n",
      "Epoch 2/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.3888 - acc: 0.3085 - val_loss: 1.3911 - val_acc: 0.2945\n",
      "Epoch 3/30\n",
      "15810/15810 [==============================] - 26s 2ms/step - loss: 1.3634 - acc: 0.3186 - val_loss: 1.3733 - val_acc: 0.2992\n",
      "Epoch 4/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.3493 - acc: 0.3330 - val_loss: 1.3714 - val_acc: 0.3290\n",
      "Epoch 5/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.3402 - acc: 0.3369 - val_loss: 1.3832 - val_acc: 0.3013\n",
      "Epoch 6/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.3336 - acc: 0.3425 - val_loss: 1.3981 - val_acc: 0.2564\n",
      "Epoch 7/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.3205 - acc: 0.3595 - val_loss: 1.4243 - val_acc: 0.3028\n",
      "Epoch 8/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.3179 - acc: 0.3590 - val_loss: 1.5299 - val_acc: 0.3216\n",
      "Epoch 9/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.3075 - acc: 0.3675 - val_loss: 1.4175 - val_acc: 0.3556\n",
      "Epoch 10/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.3047 - acc: 0.3705 - val_loss: 1.3680 - val_acc: 0.3662\n",
      "Epoch 11/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2940 - acc: 0.3796 - val_loss: 1.3915 - val_acc: 0.3546\n",
      "Epoch 12/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2904 - acc: 0.3813 - val_loss: 1.3806 - val_acc: 0.3438\n",
      "Epoch 13/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2846 - acc: 0.3876 - val_loss: 1.3794 - val_acc: 0.3127\n",
      "Epoch 14/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2814 - acc: 0.3892 - val_loss: 1.8359 - val_acc: 0.3230\n",
      "Epoch 15/30\n",
      "15810/15810 [==============================] - 26s 2ms/step - loss: 1.2752 - acc: 0.3919 - val_loss: 1.5563 - val_acc: 0.3400\n",
      "Epoch 16/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2699 - acc: 0.3968 - val_loss: 1.6694 - val_acc: 0.3351\n",
      "Epoch 17/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2607 - acc: 0.4068 - val_loss: 1.4687 - val_acc: 0.3429\n",
      "Epoch 18/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2582 - acc: 0.4087 - val_loss: 1.4395 - val_acc: 0.3372\n",
      "Epoch 19/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2563 - acc: 0.4078 - val_loss: 1.4269 - val_acc: 0.3247\n",
      "Epoch 20/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2489 - acc: 0.4156 - val_loss: 1.4804 - val_acc: 0.3510\n",
      "Epoch 21/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2459 - acc: 0.4155 - val_loss: 1.4508 - val_acc: 0.3526\n",
      "Epoch 22/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2423 - acc: 0.4178 - val_loss: 1.4826 - val_acc: 0.3425\n",
      "Epoch 23/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2353 - acc: 0.4286 - val_loss: 1.4976 - val_acc: 0.3370\n",
      "Epoch 24/30\n",
      "15810/15810 [==============================] - 26s 2ms/step - loss: 1.2299 - acc: 0.4336 - val_loss: 1.4138 - val_acc: 0.3285\n",
      "Epoch 25/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2303 - acc: 0.4308 - val_loss: 1.4129 - val_acc: 0.3383\n",
      "Epoch 26/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2217 - acc: 0.4367 - val_loss: 1.5872 - val_acc: 0.3262\n",
      "Epoch 27/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2164 - acc: 0.4419 - val_loss: 1.4708 - val_acc: 0.3241\n",
      "Epoch 28/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2151 - acc: 0.4448 - val_loss: 1.5466 - val_acc: 0.3342\n",
      "Epoch 29/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2114 - acc: 0.4412 - val_loss: 1.5042 - val_acc: 0.3245\n",
      "Epoch 30/30\n",
      "15810/15810 [==============================] - 25s 2ms/step - loss: 1.2139 - acc: 0.4414 - val_loss: 1.5817 - val_acc: 0.3402\n"
     ]
    }
   ],
   "source": [
    "num_filters = 20 #15 performs similar\n",
    "model = Sequential()\n",
    "model.add(Conv2D(num_filters, kernel_size=(num_filters,1), data_format='channels_last', input_shape=(traindata2d.shape[1], traindata2d.shape[2], traindata2d.shape[3]), kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))  \n",
    "model.add(Conv2D(num_filters, kernel_size=(15,num_filters), activation='relu', kernel_initializer='glorot_normal', bias_initializer='glorot_normal') ) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,1)))\n",
    "model.add(Permute((1,3,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(num_filters*2, kernel_size=(10,num_filters), activation='relu', kernel_initializer='glorot_normal', bias_initializer='glorot_normal')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,1)))\n",
    "model.add(Permute((1,3,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(num_filters*4, kernel_size=(10,num_filters*2), activation='relu', kernel_initializer='glorot_normal', bias_initializer='glorot_normal')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,1)))\n",
    "model.add(Permute((1,3,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(num_filters*8, kernel_size=(10,num_filters*4), activation='relu', kernel_initializer='glorot_normal', bias_initializer='glorot_normal')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,1)))\n",
    "model.add(Reshape((num_filters*8,1)))\n",
    "#model.add(LSTM(32, return_sequences=True))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='softmax', kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "hist = model.fit(traindata2d,train_labels_sliced,epochs=30,validation_split=0.25,batch_size=16)\n",
    "#model.test_on_batch(test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify test data to run model on another subject\n",
    "_, test_data_orig, _, test_labels_orig = prepare_data(file_path, \n",
    "                                            num_test_samples=50, \n",
    "                                            verbose=False, \n",
    "                                            return_all=False,\n",
    "                                            num_files=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "test_data_all = []\n",
    "test_labels_all = []\n",
    "for n in range(1,10):\n",
    "    test_data = test_data_orig['A0{}T'.format(n)]\n",
    "    test_labels = test_labels_orig['A0{}T'.format(n)]\n",
    "    #print test_data.shape\n",
    "    #print test_labels.shape\n",
    "\n",
    "    #assist numerical stability\n",
    "    test_data = test_data*(1e6)\n",
    "    test_data = test_data.swapaxes(1,2)\n",
    "    for i,a in enumerate(test_data):\n",
    "        test_data[i] = bandpass_cnt(a, 4, 38, 250, filt_order=3)    \n",
    "    #print test_data.shape\n",
    "\n",
    "    #standardize\n",
    "    for i,a in enumerate(test_data):\n",
    "        test_data[i] = exponential_running_standardize(a, factor_new=0.001, init_block_size=1000, eps=1e-4)\n",
    "\n",
    "    test_data = test_data.swapaxes(1,2) \n",
    "    test_data_sliced, test_labels_sliced = create_window_data(test_data, test_labels, windows=10)\n",
    "\n",
    "    test_data_sliced = test_data_sliced[:500,:]\n",
    "    test_labels_sliced = test_labels_sliced[:500,:]\n",
    "    #print test_data_sliced.shape\n",
    "    #print test_labels_sliced.shape \n",
    "\n",
    "    testdata2d = test_data_sliced.reshape(test_data_sliced.shape[0], test_data_sliced.shape[2], test_data_sliced.shape[1], 1)\n",
    "    #print testdata2d.shape\n",
    "    \n",
    "    test_data_all.append(testdata2d)\n",
    "    test_labels_all.append(test_labels_sliced)\n",
    "    \n",
    "print(len(test_data_all))\n",
    "print(len(test_labels_all))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 459us/step\n",
      "Test Score: [1.764169979572296, 0.38200000000000001]\n",
      "500/500 [==============================] - 0s 403us/step\n",
      "Test Score: [1.7105364065170288, 0.29799999999999999]\n",
      "500/500 [==============================] - 0s 409us/step\n",
      "Test Score: [1.6860965385437012, 0.34399999999999997]\n",
      "500/500 [==============================] - 0s 399us/step\n",
      "Test Score: [1.5207793159484864, 0.374]\n",
      "500/500 [==============================] - 0s 407us/step\n",
      "Test Score: [2.3204163894653322, 0.19]\n",
      "500/500 [==============================] - 0s 394us/step\n",
      "Test Score: [2.042624954223633, 0.26200000000000001]\n",
      "500/500 [==============================] - 0s 393us/step\n",
      "Test Score: [2.1792755298614503, 0.27000000000000002]\n",
      "500/500 [==============================] - 0s 395us/step\n",
      "Test Score: [2.5227179417610168, 0.23999999999999999]\n",
      "500/500 [==============================] - 0s 411us/step\n",
      "Test Score: [3.2447946605682372, 0.28000000000000003]\n"
     ]
    }
   ],
   "source": [
    "for n in range(0,9):\n",
    "    testdata2d = test_data_all[n]\n",
    "    test_labels_sliced = test_labels_all[n]\n",
    "    test_score = model.evaluate(testdata2d, test_labels_sliced, batch_size=16)\n",
    "    print \"Test Score: {}\".format(test_score)\n",
    "    #plot_hist([hist.history['acc'],hist.history['val_acc']],['Training Accuracy','Val Accuracy'],title='Accuracies')\n",
    "    #plot_hist([hist.history['loss'],hist.history['val_loss']],['Training Loss','Val Loss'],title='Loss')\n",
    "\n",
    "    #test_predict = model.predict(testdata2d, batch_size=16)\n",
    "    #cm = confusion_matrix(np.argmax(test_labels_sliced,axis=1),np.argmax(test_predict,axis=1))\n",
    "    #unique, count = np.unique(np.argmax(test_labels_sliced,axis=1),return_counts=True)\n",
    "    #print count \n",
    "    #plt.figure(0)\n",
    "    #plot_confusion_matrix(cm, classes=[\"1\",\"2\",\"3\",\"4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21080, 512, 22, 1)\n"
     ]
    }
   ],
   "source": [
    "print traindata2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
