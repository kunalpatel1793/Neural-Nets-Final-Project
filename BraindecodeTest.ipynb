{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras.layers import ConvLSTM2D, Conv2D, MaxPooling2D\n",
    "from keras.layers import MaxPooling1D, Conv1D\n",
    "from keras.layers import GRU, LSTM, BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from UtilNNDL import create_window_data\n",
    "from UtilNNDL import plot_hist\n",
    "from UtilNNDL import prepare_data\n",
    "\n",
    "A01T = h5py.File('/home/carla/Downloads/project_datasets/project_datasets/A01T_slice.mat','r')\n",
    "data = np.copy(A01T['image'])\n",
    "labels = np.copy(A01T['type'])\n",
    "labels = labels[0,0:data.shape[0]:1]\n",
    "labels = np.asarray(labels, dtype=np.int32)\n",
    "\n",
    "a = data[:56]\n",
    "b = data[57:]\n",
    "data = np.vstack((a,b))\n",
    "a = labels[:56]\n",
    "b = labels[57:]\n",
    "labels = np.hstack((a,b))\n",
    "#enc = OneHotEncoder()\n",
    "#enc_labels = enc.fit_transform(labels.reshape(-1,1)).toarray()\n",
    "enc_labels = to_categorical(labels-769, num_classes=4)\n",
    "print(enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2108, 22, 1000)\n",
      "(2108, 4)\n",
      "(450, 22, 1000)\n",
      "(450, 4)\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/carla/Downloads/project_datasets/project_datasets/'\n",
    "train_data, test_data, train_labels, test_labels = prepare_data(file_path, \n",
    "                                                                num_test_samples = 50, \n",
    "                                                                verbose= False, \n",
    "                                                                return_all=True)\n",
    "print train_data.shape\n",
    "print train_labels.shape\n",
    "print test_data.shape\n",
    "print test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 22, 1000)\n",
      "(50, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "bs, t, f = data.shape\n",
    "np.random.seed(42)\n",
    "shuffle = np.random.choice(bs,bs,replace=False)\n",
    "\n",
    "data = data[:,0:22,:]\n",
    "\n",
    "train_samples = data.shape[0]-50\n",
    "train_data = data[shuffle[:train_samples],:,:]\n",
    "train_labels = enc_labels[shuffle[:train_samples]]\n",
    "test_data = data[shuffle[train_samples:],:,:]\n",
    "test_labels = enc_labels[shuffle[train_samples:]]\n",
    "print train_data.shape\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(474, 22, 1000)\n",
      "(100, 22, 1000)\n",
      "(474, 4)\n",
      "(100, 4)\n"
     ]
    }
   ],
   "source": [
    "#Experimental, try flipping the data in time\n",
    "train_data = np.vstack([train_data, train_data[:,:,::-1]])\n",
    "test_data = np.vstack([test_data, test_data[:,:,::-1]]) \n",
    "print train_data.shape\n",
    "print test_data.shape\n",
    "train_labels = np.vstack([train_labels, train_labels])\n",
    "test_labels = np.vstack([test_labels, test_labels])\n",
    "print train_labels.shape\n",
    "print test_labels .shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21080, 22, 512)\n",
      "(21080, 4)\n",
      "(4500, 22, 512)\n",
      "(4500, 4)\n"
     ]
    }
   ],
   "source": [
    "#Augment the data into a bigger set by windowing\n",
    "train_data_sliced, train_labels_sliced = create_window_data(train_data, train_labels, windows=10)\n",
    "test_data_sliced, test_labels_sliced = create_window_data(test_data, test_labels, windows=10)\n",
    "print train_data_sliced.shape\n",
    "print train_labels_sliced.shape\n",
    "print test_data_sliced.shape\n",
    "print test_labels_sliced.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 22, 512)\n",
      "(500, 4)\n"
     ]
    }
   ],
   "source": [
    "test_data_sliced = test_data_sliced[:500,:]\n",
    "test_labels_sliced = test_labels_sliced[:500,:]\n",
    "print test_data_sliced.shape\n",
    "print test_labels_sliced.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21080, 512, 22, 1)\n",
      "(500, 512, 22, 1)\n"
     ]
    }
   ],
   "source": [
    "traindata2d = train_data_sliced.reshape(train_data_sliced.shape[0], train_data_sliced.shape[2], train_data_sliced.shape[1], 1)\n",
    "testdata2d = test_data_sliced.reshape(test_data_sliced.shape[0], test_data_sliced.shape[2], test_data_sliced.shape[1], 1)\n",
    "print traindata2d.shape\n",
    "print testdata2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 491, 22, 40)       920       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 491, 1, 40)        35240     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 491, 1, 40)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 1, 40)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 1284      \n",
      "=================================================================\n",
      "Total params: 37,604\n",
      "Trainable params: 37,524\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "Train on 15810 samples, validate on 5270 samples\n",
      "Epoch 1/20\n",
      "15810/15810 [==============================] - 9s 592us/step - loss: 2.8418 - acc: 0.2684 - val_loss: 1.5639 - val_acc: 0.3169\n",
      "Epoch 2/20\n",
      "15810/15810 [==============================] - 9s 575us/step - loss: 1.5810 - acc: 0.2877 - val_loss: 1.4263 - val_acc: 0.3127\n",
      "Epoch 3/20\n",
      "15810/15810 [==============================] - 9s 573us/step - loss: 1.4367 - acc: 0.3049 - val_loss: 1.4220 - val_acc: 0.2854\n",
      "Epoch 4/20\n",
      "15810/15810 [==============================] - 9s 573us/step - loss: 1.4096 - acc: 0.3139 - val_loss: 1.4363 - val_acc: 0.3072\n",
      "Epoch 5/20\n",
      "15810/15810 [==============================] - 9s 576us/step - loss: 1.3924 - acc: 0.3216 - val_loss: 1.4466 - val_acc: 0.2774\n",
      "Epoch 6/20\n",
      "15810/15810 [==============================] - 9s 574us/step - loss: 1.3808 - acc: 0.3298 - val_loss: 1.5252 - val_acc: 0.3178\n",
      "Epoch 7/20\n",
      "15810/15810 [==============================] - 9s 574us/step - loss: 1.3727 - acc: 0.3336 - val_loss: 1.4073 - val_acc: 0.3101\n",
      "Epoch 8/20\n",
      "15810/15810 [==============================] - 9s 573us/step - loss: 1.3649 - acc: 0.3425 - val_loss: 1.3952 - val_acc: 0.3277\n",
      "Epoch 9/20\n",
      "15810/15810 [==============================] - 9s 575us/step - loss: 1.3602 - acc: 0.3457 - val_loss: 1.4490 - val_acc: 0.3091\n",
      "Epoch 10/20\n",
      "15810/15810 [==============================] - 9s 575us/step - loss: 1.3493 - acc: 0.3531 - val_loss: 1.3684 - val_acc: 0.3315\n",
      "Epoch 11/20\n",
      "15810/15810 [==============================] - 9s 573us/step - loss: 1.3464 - acc: 0.3602 - val_loss: 1.4013 - val_acc: 0.3446\n",
      "Epoch 12/20\n",
      "15810/15810 [==============================] - 9s 576us/step - loss: 1.3402 - acc: 0.3669 - val_loss: 1.3959 - val_acc: 0.3389\n",
      "Epoch 13/20\n",
      "15810/15810 [==============================] - 9s 575us/step - loss: 1.3342 - acc: 0.3737 - val_loss: 1.3875 - val_acc: 0.3472\n",
      "Epoch 14/20\n",
      "15810/15810 [==============================] - 9s 575us/step - loss: 1.3335 - acc: 0.3729 - val_loss: 1.4271 - val_acc: 0.3277\n",
      "Epoch 15/20\n",
      "15810/15810 [==============================] - 9s 575us/step - loss: 1.3289 - acc: 0.3837 - val_loss: 1.4035 - val_acc: 0.3353\n",
      "Epoch 16/20\n",
      "15810/15810 [==============================] - 9s 572us/step - loss: 1.3230 - acc: 0.3829 - val_loss: 1.3565 - val_acc: 0.3565\n",
      "Epoch 17/20\n",
      "15810/15810 [==============================] - 9s 577us/step - loss: 1.3153 - acc: 0.3930 - val_loss: 1.4125 - val_acc: 0.3450\n",
      "Epoch 18/20\n",
      "15810/15810 [==============================] - 9s 575us/step - loss: 1.3109 - acc: 0.3967 - val_loss: 1.5019 - val_acc: 0.3209\n",
      "Epoch 19/20\n",
      "15810/15810 [==============================] - 9s 573us/step - loss: 1.3047 - acc: 0.4076 - val_loss: 1.4148 - val_acc: 0.3429\n",
      "Epoch 20/20\n",
      "15810/15810 [==============================] - 9s 574us/step - loss: 1.3034 - acc: 0.4090 - val_loss: 1.4371 - val_acc: 0.3472\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 4500 input samples and 500 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a1f647d01bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata2d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels_sliced\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_sliced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Test Score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mplot_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Val Accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Accuracies'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carla/Documents/tensorflow/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                    steps=steps)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carla/Documents/tensorflow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1776\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carla/Documents/tensorflow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m             \u001b[0m_check_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1499\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carla/Documents/tensorflow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    218\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 4500 input samples and 500 target samples."
     ]
    }
   ],
   "source": [
    "# Conv2D + LSTM\n",
    "\"\"\"\n",
    "cnn = Sequential()\n",
    "cnn.add(ConvLSTM2D(1, (2,2), strides=2, activation='relu',input_shape=(1000, 5, 5, 1)))\n",
    "cnn.add(Flatten())\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist = cnn.fit(train_data,train_labels,epochs=5,validation_split=0.25,batch_size=32)\n",
    "print(model.summary())\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(40, kernel_size=(22,1), activation='relu', data_format='channels_last', input_shape=(512, 22, 1))) \n",
    "model.add(Conv2D(40, kernel_size=(1,22), activation='relu') ) \n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(60,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.65))\n",
    "model.add(Dense(4, activation='softmax', kernel_regularizer=regularizers.l2(0.05)))\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "hist = model.fit(traindata2d,train_labels_sliced,epochs=20,validation_split=0.25,batch_size=32)\n",
    "test_score = model.evaluate(testdata2d, test_labels_sliced, batch_size=32)\n",
    "print \"Test Score: {}\".format(test_score)\n",
    "plot_hist([hist.history['acc'],hist.history['val_acc']],['Training Accuracy','Val Accuracy'],title='Accuracies')\n",
    "plot_hist([hist.history['loss'],hist.history['val_loss']],['Training Loss','Val Loss'],title='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
